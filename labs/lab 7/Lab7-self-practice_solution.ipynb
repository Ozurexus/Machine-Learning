{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-6 : Self-Practice\n",
    "\n",
    "In this week self-practice, you will implement a neural network model for a regression problem. You will use the [*admission*](./Admission_Predict.csv) dataset attached, used in the previous lab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset and do all the necessary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.807500</td>\n",
       "      <td>107.410000</td>\n",
       "      <td>3.087500</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.452500</td>\n",
       "      <td>8.598925</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.724350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.473646</td>\n",
       "      <td>6.069514</td>\n",
       "      <td>1.143728</td>\n",
       "      <td>1.006869</td>\n",
       "      <td>0.898478</td>\n",
       "      <td>0.596317</td>\n",
       "      <td>0.498362</td>\n",
       "      <td>0.142609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.610000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.062500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GRE Score  TOEFL Score  University Rating         SOP        LOR   \\\n",
       "count  400.000000   400.000000         400.000000  400.000000  400.000000   \n",
       "mean   316.807500   107.410000           3.087500    3.400000    3.452500   \n",
       "std     11.473646     6.069514           1.143728    1.006869    0.898478   \n",
       "min    290.000000    92.000000           1.000000    1.000000    1.000000   \n",
       "25%    308.000000   103.000000           2.000000    2.500000    3.000000   \n",
       "50%    317.000000   107.000000           3.000000    3.500000    3.500000   \n",
       "75%    325.000000   112.000000           4.000000    4.000000    4.000000   \n",
       "max    340.000000   120.000000           5.000000    5.000000    5.000000   \n",
       "\n",
       "             CGPA    Research  Chance of Admit   \n",
       "count  400.000000  400.000000        400.000000  \n",
       "mean     8.598925    0.547500          0.724350  \n",
       "std      0.596317    0.498362          0.142609  \n",
       "min      6.800000    0.000000          0.340000  \n",
       "25%      8.170000    0.000000          0.640000  \n",
       "50%      8.610000    1.000000          0.730000  \n",
       "75%      9.062500    1.000000          0.830000  \n",
       "max      9.920000    1.000000          0.970000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Admission_Predict.csv')\n",
    "df.head(5)\n",
    "\n",
    "df = df.drop(['Serial No.'], axis=1)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "y  = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scale all the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create custom pytorch `Dataset`\n",
    "\n",
    "You should create a class `CustomDataset` that inherits  the abstract class `torch.utils.data.Dataset` from pytorch. \n",
    "\n",
    "> **Note** You should overwrite `__getitem__`, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite `__len__`, which is expected to return the size of the dataset by many `~torch.utils.data.Sampler` implementations and the default options of `~torch.utils.data.DataLoader`.\n",
    "\n",
    "#### Split your dataset into train and test data loaders\n",
    "You can create a `CustomDataset` instance with the entire dataframe and use [`random_split`](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) to split it into training and testing datasets. And then, create test and train dataloader. Or you can split using `train_test_split` from sklearn and past the splitted sets to your Custom dataset class. \n",
    "\n",
    "Create train and test dataloader with `batch_size = 32` each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class CustumData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.y = torch.tensor(y).float()\n",
    "        self.X = torch.tensor(X).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx, :], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 7])\n",
      "torch.Size([80, 7])\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustumData(X_train, y_train)\n",
    "test_dataset = CustumData(X_test, y_test) \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, 32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, 32, shuffle=False)\n",
    "print(train_dataset.X.shape)\n",
    "print(test_dataset.X.shape)\n",
    "print(train_dataset.y.dtype)\n",
    "print(test_dataset.y.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = next(iter(train_dataloader))\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "Using `nn`, Create a neural network with 1 hidden layers of size 100, each must be followed by a `leaky_relu` activation function and define the forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_hidden_unit = 100):\n",
    "        super(Net, self).__init__()\n",
    "        # Write 3 lines to define 3 more linear layers.\n",
    "        # 2 hidden layers with number of neurons numbers: 250 and 100\n",
    "        # 1 output layer that should output 10 neurons, one for each class.\n",
    "        self.fc1 = nn.Linear(7, n_hidden_unit) \n",
    "        self.fc2 = nn.Linear(n_hidden_unit, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # the linear layers fc1, fc2, fc3, and fc4\n",
    "        # accepts only flattened input (1D batches)\n",
    "        # while the batch x is of size (batch, 28 * 28)\n",
    "        # define one line to flatten the x to be of size (batch_sz, 28 * 28)\n",
    "        \n",
    "        #x = F.sigmoid(self.fc1(x))\n",
    "        #x = F.sigmoid(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x)) \n",
    "        return x\n",
    "\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\")\n",
    "model = Net(n_hidden_unit = 100).to(device)\n",
    "#model = nn.Sequential(nn.Linear(7, 1)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Define the appropriate loss function and the training loop for the training and the testing dataloader (as done in the lab). Print the final loss on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 2\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(batch_idx)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data).squeeze()  \n",
    "        loss = loss_fn(output, target) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Do the same that was done in the previous function.\n",
    "            # But without backprobagating the loss and without running the optimizers\n",
    "            # As this function is only for test.\n",
    "            # write 3 lines to transform the data to the device, get the output and compute the loss\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data).squeeze() \n",
    "            test_loss += loss_fn(output, target).item()  # sum up batch loss \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
    "        test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/320 (0%)]\tLoss: 0.538640\n",
      "Train Epoch: 1 [64/320 (20%)]\tLoss: 0.457457\n",
      "Train Epoch: 1 [128/320 (40%)]\tLoss: 0.477153\n",
      "Train Epoch: 1 [192/320 (60%)]\tLoss: 0.432072\n",
      "Train Epoch: 1 [256/320 (80%)]\tLoss: 0.290378\n",
      "\n",
      "Test set: Average loss: 0.0060\n",
      "\n",
      "Train Epoch: 2 [0/320 (0%)]\tLoss: 0.146292\n",
      "Train Epoch: 2 [64/320 (20%)]\tLoss: 0.109488\n",
      "Train Epoch: 2 [128/320 (40%)]\tLoss: 0.084112\n",
      "Train Epoch: 2 [192/320 (60%)]\tLoss: 0.070336\n",
      "Train Epoch: 2 [256/320 (80%)]\tLoss: 0.058007\n",
      "\n",
      "Test set: Average loss: 0.0024\n",
      "\n",
      "Train Epoch: 3 [0/320 (0%)]\tLoss: 0.057938\n",
      "Train Epoch: 3 [64/320 (20%)]\tLoss: 0.065086\n",
      "Train Epoch: 3 [128/320 (40%)]\tLoss: 0.056503\n",
      "Train Epoch: 3 [192/320 (60%)]\tLoss: 0.064579\n",
      "Train Epoch: 3 [256/320 (80%)]\tLoss: 0.048636\n",
      "\n",
      "Test set: Average loss: 0.0016\n",
      "\n",
      "Train Epoch: 4 [0/320 (0%)]\tLoss: 0.037717\n",
      "Train Epoch: 4 [64/320 (20%)]\tLoss: 0.054633\n",
      "Train Epoch: 4 [128/320 (40%)]\tLoss: 0.042061\n",
      "Train Epoch: 4 [192/320 (60%)]\tLoss: 0.034922\n",
      "Train Epoch: 4 [256/320 (80%)]\tLoss: 0.037735\n",
      "\n",
      "Test set: Average loss: 0.0012\n",
      "\n",
      "Train Epoch: 5 [0/320 (0%)]\tLoss: 0.027969\n",
      "Train Epoch: 5 [64/320 (20%)]\tLoss: 0.028869\n",
      "Train Epoch: 5 [128/320 (40%)]\tLoss: 0.034003\n",
      "Train Epoch: 5 [192/320 (60%)]\tLoss: 0.032898\n",
      "Train Epoch: 5 [256/320 (80%)]\tLoss: 0.025559\n",
      "\n",
      "Test set: Average loss: 0.0008\n",
      "\n",
      "Train Epoch: 6 [0/320 (0%)]\tLoss: 0.040496\n",
      "Train Epoch: 6 [64/320 (20%)]\tLoss: 0.015293\n",
      "Train Epoch: 6 [128/320 (40%)]\tLoss: 0.016915\n",
      "Train Epoch: 6 [192/320 (60%)]\tLoss: 0.021117\n",
      "Train Epoch: 6 [256/320 (80%)]\tLoss: 0.025762\n",
      "\n",
      "Test set: Average loss: 0.0006\n",
      "\n",
      "Train Epoch: 7 [0/320 (0%)]\tLoss: 0.022124\n",
      "Train Epoch: 7 [64/320 (20%)]\tLoss: 0.014253\n",
      "Train Epoch: 7 [128/320 (40%)]\tLoss: 0.021532\n",
      "Train Epoch: 7 [192/320 (60%)]\tLoss: 0.012135\n",
      "Train Epoch: 7 [256/320 (80%)]\tLoss: 0.017287\n",
      "\n",
      "Test set: Average loss: 0.0005\n",
      "\n",
      "Train Epoch: 8 [0/320 (0%)]\tLoss: 0.017144\n",
      "Train Epoch: 8 [64/320 (20%)]\tLoss: 0.012460\n",
      "Train Epoch: 8 [128/320 (40%)]\tLoss: 0.021608\n",
      "Train Epoch: 8 [192/320 (60%)]\tLoss: 0.016584\n",
      "Train Epoch: 8 [256/320 (80%)]\tLoss: 0.010415\n",
      "\n",
      "Test set: Average loss: 0.0004\n",
      "\n",
      "Train Epoch: 9 [0/320 (0%)]\tLoss: 0.009664\n",
      "Train Epoch: 9 [64/320 (20%)]\tLoss: 0.012572\n",
      "Train Epoch: 9 [128/320 (40%)]\tLoss: 0.016148\n",
      "Train Epoch: 9 [192/320 (60%)]\tLoss: 0.012147\n",
      "Train Epoch: 9 [256/320 (80%)]\tLoss: 0.007160\n",
      "\n",
      "Test set: Average loss: 0.0003\n",
      "\n",
      "Train Epoch: 10 [0/320 (0%)]\tLoss: 0.014916\n",
      "Train Epoch: 10 [64/320 (20%)]\tLoss: 0.008518\n",
      "Train Epoch: 10 [128/320 (40%)]\tLoss: 0.013597\n",
      "Train Epoch: 10 [192/320 (60%)]\tLoss: 0.012299\n",
      "Train Epoch: 10 [256/320 (80%)]\tLoss: 0.007744\n",
      "\n",
      "Test set: Average loss: 0.0003\n",
      "\n",
      "Train Epoch: 11 [0/320 (0%)]\tLoss: 0.008312\n",
      "Train Epoch: 11 [64/320 (20%)]\tLoss: 0.007872\n",
      "Train Epoch: 11 [128/320 (40%)]\tLoss: 0.012585\n",
      "Train Epoch: 11 [192/320 (60%)]\tLoss: 0.012269\n",
      "Train Epoch: 11 [256/320 (80%)]\tLoss: 0.010859\n",
      "\n",
      "Test set: Average loss: 0.0003\n",
      "\n",
      "Train Epoch: 12 [0/320 (0%)]\tLoss: 0.006364\n",
      "Train Epoch: 12 [64/320 (20%)]\tLoss: 0.007801\n",
      "Train Epoch: 12 [128/320 (40%)]\tLoss: 0.007328\n",
      "Train Epoch: 12 [192/320 (60%)]\tLoss: 0.009804\n",
      "Train Epoch: 12 [256/320 (80%)]\tLoss: 0.009820\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 13 [0/320 (0%)]\tLoss: 0.008773\n",
      "Train Epoch: 13 [64/320 (20%)]\tLoss: 0.010921\n",
      "Train Epoch: 13 [128/320 (40%)]\tLoss: 0.004330\n",
      "Train Epoch: 13 [192/320 (60%)]\tLoss: 0.006049\n",
      "Train Epoch: 13 [256/320 (80%)]\tLoss: 0.008514\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 14 [0/320 (0%)]\tLoss: 0.007674\n",
      "Train Epoch: 14 [64/320 (20%)]\tLoss: 0.007206\n",
      "Train Epoch: 14 [128/320 (40%)]\tLoss: 0.007050\n",
      "Train Epoch: 14 [192/320 (60%)]\tLoss: 0.005964\n",
      "Train Epoch: 14 [256/320 (80%)]\tLoss: 0.010762\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 15 [0/320 (0%)]\tLoss: 0.007058\n",
      "Train Epoch: 15 [64/320 (20%)]\tLoss: 0.004371\n",
      "Train Epoch: 15 [128/320 (40%)]\tLoss: 0.008602\n",
      "Train Epoch: 15 [192/320 (60%)]\tLoss: 0.010681\n",
      "Train Epoch: 15 [256/320 (80%)]\tLoss: 0.008999\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 16 [0/320 (0%)]\tLoss: 0.005075\n",
      "Train Epoch: 16 [64/320 (20%)]\tLoss: 0.008370\n",
      "Train Epoch: 16 [128/320 (40%)]\tLoss: 0.009467\n",
      "Train Epoch: 16 [192/320 (60%)]\tLoss: 0.005889\n",
      "Train Epoch: 16 [256/320 (80%)]\tLoss: 0.007638\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 17 [0/320 (0%)]\tLoss: 0.007717\n",
      "Train Epoch: 17 [64/320 (20%)]\tLoss: 0.004315\n",
      "Train Epoch: 17 [128/320 (40%)]\tLoss: 0.005891\n",
      "Train Epoch: 17 [192/320 (60%)]\tLoss: 0.007762\n",
      "Train Epoch: 17 [256/320 (80%)]\tLoss: 0.008112\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 18 [0/320 (0%)]\tLoss: 0.008760\n",
      "Train Epoch: 18 [64/320 (20%)]\tLoss: 0.005758\n",
      "Train Epoch: 18 [128/320 (40%)]\tLoss: 0.007294\n",
      "Train Epoch: 18 [192/320 (60%)]\tLoss: 0.004974\n",
      "Train Epoch: 18 [256/320 (80%)]\tLoss: 0.005279\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 19 [0/320 (0%)]\tLoss: 0.005374\n",
      "Train Epoch: 19 [64/320 (20%)]\tLoss: 0.004904\n",
      "Train Epoch: 19 [128/320 (40%)]\tLoss: 0.004819\n",
      "Train Epoch: 19 [192/320 (60%)]\tLoss: 0.008854\n",
      "Train Epoch: 19 [256/320 (80%)]\tLoss: 0.007160\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n",
      "Train Epoch: 20 [0/320 (0%)]\tLoss: 0.006138\n",
      "Train Epoch: 20 [64/320 (20%)]\tLoss: 0.005060\n",
      "Train Epoch: 20 [128/320 (40%)]\tLoss: 0.007079\n",
      "Train Epoch: 20 [192/320 (60%)]\tLoss: 0.005032\n",
      "Train Epoch: 20 [256/320 (80%)]\tLoss: 0.006575\n",
      "\n",
      "Test set: Average loss: 0.0002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "zy_pred_nn = model(torch.tensor(X_test).float()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare your Neural network model to a Linear Regression\n",
    "Train a simple linear regression model on the training set and print MSE on the testing set (`X_test`). Also print the MSE on the test set using the your neural model. \n",
    "\n",
    "> Compare the results (which performs best) and justify why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE neural network 0.004361042830272186\n",
      "MSE Linear regressionn 0.003332077754891382\n"
     ]
    }
   ],
   "source": [
    "#import detach\n",
    "from torch import detach\n",
    "print(\"MSE neural network\", mean_squared_error(y_test, detach(zy_pred_nn).numpy()))\n",
    "print(\"MSE Linear regressionn\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
